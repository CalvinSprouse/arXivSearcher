{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import json\n",
    "import requests\n",
    "import time\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from rich import print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "# custom headers to mimic browser\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (X11; Ubuntu; Linux i686; rv:48.0) Gecko/20100101 Firefox/48.0\",}\n",
    "\n",
    "# search url, append with url_num to access a specific page of items\n",
    "astro_ph_url_base = r\"https://arxiv.org/search/advanced?advanced=&terms-0-operator=AND&terms-0-term=&terms-0-field=title&classification-physics=y&classification-physics_archives=astro-ph&classification-include_cross_list=include&date-year=&date-filter_by=date_range&date-from_date=2022-07&date-to_date=&date-date_type=submitted_date&abstracts=show&size=200&order=-announced_date_first&start=\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape the website for its articles and get a list with title and link\n",
    "def recursive_page_scrape(base_url: str, url_num: int = 0, interval_timer: int = 10, do_outputs: bool = False, _articles_list: list = [], _already_failed: bool = False):\n",
    "    # the program should always give an output, http responses fail sometimes\n",
    "    try:\n",
    "        # make initial request\n",
    "        request = requests.get(astro_ph_url_base + str(url_num), headers=headers)\n",
    "\n",
    "        # build base soup\n",
    "        soup = BeautifulSoup(request.content, \"html.parser\")\n",
    "\n",
    "        # get a list of the article results\n",
    "        articles = soup.find_all(\"li\", {\"class\": \"arxiv-result\"})\n",
    "\n",
    "        # only parse articles if there are any otherwise return the full list\n",
    "        if len(articles) <= 0: return _articles_list\n",
    "\n",
    "        # iterate over list of articles\n",
    "        for article in articles:\n",
    "            # get data from html tag\n",
    "            title = article.find(\"p\", {\"class\": \"title\"}).text.strip()\n",
    "            pdf_link = [l[\"href\"] for l in article.find_all(\"a\") if \"pdf\" in l.text]\n",
    "\n",
    "            # skip article if no pdf links are present\n",
    "            if len(pdf_link) != 1: continue\n",
    "\n",
    "            # extract single link and append results to list\n",
    "            pdf_link = pdf_link[0]\n",
    "            _articles_list.append((title, pdf_link))\n",
    "\n",
    "        # recurse and call back with incremented url_num\n",
    "        if do_outputs: print(f\"Page scraped from {url_num} to {url_num + len(articles)} results. Scraping resumes after {interval_timer} second sleep.\")\n",
    "        time.sleep(interval_timer)\n",
    "        return recursive_page_scrape(base_url=base_url,\n",
    "                                     url_num=url_num+len(articles),\n",
    "                                     interval_timer=interval_timer,\n",
    "                                     do_outputs=do_outputs,\n",
    "                                     _articles_list=_articles_list,\n",
    "                                     _already_failed=_already_failed)\n",
    "    except Exception as e:\n",
    "        # recall the code with a already_failed flag set to true, if it fails twice kill it\n",
    "        if do_outputs: print(f\"Exiting due to exception {e}.\")\n",
    "        return _articles_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the list of articles\n",
    "articles = recursive_page_scrape(base_url=astro_ph_url_base, do_outputs=True, interval_timer=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9996</span> articles saved\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m9996\u001b[0m articles saved\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# save the list of articles to a file\n",
    "# first the list has to be made into a dict, the key will just be the number of the article\n",
    "saved_articles_dict = [{num: {\"title\": art[0], \"link\": art[1]}} for num, art in enumerate(articles)]\n",
    "print(f\"{len(saved_articles_dict)} articles saved\")\n",
    "# print(json.dumps(saved_articles_dict))\n",
    "\n",
    "# open/make a new file to dump to\n",
    "with open(\"articles_list.json\", \"w+\") as writer: json.dump(saved_articles_dict, writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4036b6f07bf01c04c62e0d0cbf6b71295a82f54724682726afee683bb5a9c458"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
