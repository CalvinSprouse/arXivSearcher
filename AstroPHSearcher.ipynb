{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import json\n",
    "import re\n",
    "import requests\n",
    "import os\n",
    "import time\n",
    "import webbrowser\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from rich import print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "# custom headers to mimic browser\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (X11; Ubuntu; Linux i686; rv:48.0) Gecko/20100101 Firefox/48.0\",}\n",
    "\n",
    "# search url, append with url_num to access a specific page of items\n",
    "astro_ph_url_base = r\"https://arxiv.org/search/advanced?advanced=&terms-0-operator=AND&terms-0-term=&terms-0-field=title&classification-physics=y&classification-physics_archives=astro-ph&classification-include_cross_list=include&date-year=&date-filter_by=date_range&date-from_date=2022-07&date-to_date=&date-date_type=submitted_date&abstracts=show&size=200&order=-announced_date_first&start=\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape the website for its articles and get a list with title and link\n",
    "def recursive_page_scrape(base_url: str, url_num: int = 0, interval_timer: int = 10, do_outputs: bool = False,\n",
    "                          articles_list: list = [], already_failed: bool = False, recursed: bool = False):\n",
    "    # reset articles list at the start of the function\n",
    "    if not recursed: articles_list = []\n",
    "\n",
    "    # the program should always give an output, http responses fail sometimes\n",
    "    try:\n",
    "        # make initial request\n",
    "        request = requests.get(astro_ph_url_base + str(url_num), headers=headers)\n",
    "\n",
    "        # build base soup\n",
    "        soup = BeautifulSoup(request.content, \"html.parser\")\n",
    "\n",
    "        # get a list of the article results\n",
    "        articles = soup.find_all(\"li\", {\"class\": \"arxiv-result\"})\n",
    "\n",
    "        # only parse articles if there are any otherwise return the full list\n",
    "        if len(articles) <= 0: return articles_list\n",
    "\n",
    "        # iterate over list of articles\n",
    "        for article in articles:\n",
    "            # get data from html tag\n",
    "            title = article.find(\"p\", {\"class\": \"title\"}).text.strip()\n",
    "            pdf_link = [l[\"href\"] for l in article.find_all(\"a\") if \"pdf\" in l.text]\n",
    "\n",
    "            # skip article if no pdf links are present\n",
    "            if len(pdf_link) != 1: continue\n",
    "\n",
    "            # only include stellar astro articles (optional)\n",
    "            if \"astro-ph.SR\" not in str(article): continue\n",
    "\n",
    "            # extract single link and append results to list\n",
    "            pdf_link = pdf_link[0]\n",
    "            articles_list.append((title, pdf_link))\n",
    "\n",
    "        # recurse and call back with incremented url_num\n",
    "        if do_outputs: print(f\"Page scraped from {url_num} to {url_num + len(articles)} results. Scraping resumes after {interval_timer} second sleep.\")\n",
    "        time.sleep(interval_timer)\n",
    "        return recursive_page_scrape(base_url=base_url,\n",
    "                                     url_num=url_num+len(articles),\n",
    "                                     interval_timer=interval_timer,\n",
    "                                     do_outputs=do_outputs,\n",
    "                                     articles_list=articles_list,\n",
    "                                     already_failed=already_failed,\n",
    "                                     recursed=True)\n",
    "    except Exception as e:\n",
    "        # recall the code with a already_failed flag set to true, if it fails twice kill it\n",
    "        if do_outputs: print(f\"Exiting due to exception {e}.\")\n",
    "        return articles_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Scraped <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2397</span> articles.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Scraped \u001b[1;36m2397\u001b[0m articles.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get the list of articles\n",
    "articles = recursive_page_scrape(base_url=astro_ph_url_base, do_outputs=False, interval_timer=0)\n",
    "print(f\"Scraped {len(articles)} articles.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2397</span> articles saved\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2397\u001b[0m articles saved\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# save the list of articles to a file\n",
    "# first the list has to be made into a dict, the key will just be the number of the article\n",
    "saved_articles_dict = [{num: {\"title\": art[0], \"link\": art[1]}} for num, art in enumerate(articles)]\n",
    "print(f\"{len(saved_articles_dict)} articles saved\")\n",
    "# print(json.dumps(saved_articles_dict))\n",
    "\n",
    "# open/make a new file to dump to\n",
    "with open(\"sa_articles_list.json\", \"w+\") as writer: json.dump(saved_articles_dict, writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and save into a folder\n",
    "article_folder = \"Articles\"\n",
    "os.makedirs(article_folder, exist_ok=True)\n",
    "\n",
    "# download and save all articles\n",
    "for number, article in enumerate(articles):\n",
    "    # define a new title\n",
    "    new_title = re.sub(r\"\\$\\\\.*{([^}]*)}\\$\", r\"\\g<1>\", str(article[0])).replace(\" \", \"_\").replace(\"$\", \"\").replace(\"/\", \"-\").replace(\"\\\\\", \"-\") + \".txt\"\n",
    "\n",
    "    # download pdf\n",
    "    # r = requests.get(article[1], stream=True)\n",
    "    # with open(os.path.join(article_folder, new_title), \"wb\") as download: download.write(r.content)\n",
    "    # save as text file with link for sorting\n",
    "    with open(os.path.join(article_folder, new_title), \"w\") as writer: writer.write(article[1])\n",
    "\n",
    "    # report\n",
    "    print(f\"{number}: Downloaded {new_title}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply sorts\n",
    "gravi_wave_articles = [_ for _ in articles if \"gravi\" in _[0] and \"wave\" in _[0]]\n",
    "\n",
    "for a in gravi_wave_articles: webbrowser.open(a[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8 (v3.9.8:bb3fdcfe95, Nov  5 2021, 16:40:46) \n[Clang 13.0.0 (clang-1300.0.29.3)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4036b6f07bf01c04c62e0d0cbf6b71295a82f54724682726afee683bb5a9c458"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
